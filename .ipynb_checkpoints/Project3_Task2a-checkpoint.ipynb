{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bb27a5",
   "metadata": {},
   "source": [
    "## Task 2a\n",
    "Create cluster-irrelevant features in the data by e.g. perturbing features or replace with simulated noise features. Note, it is probably easier if you first filter out some cluster relevant features so you have an informative set to start from and keep these intact, but you are free to investigate the question of noisy features in other ways if you prefer.\n",
    "\n",
    "Investigate how an increasing proportion of cluster-irrelevant features impact clustering and the selection of the number of clusters. Is this sensitive to how you choose to reduce dimensions or select features?\n",
    "\n",
    "To make this interesting, explore at least 2-3 dimension reduction techniques. You can consider feature selection methods, projections, factorization methods or auto-encoders for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c825fe",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ce8867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Loads the data\n",
    "TCGAData = pyreadr.read_r('TCGAdata.RData') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53369963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BC' 'GBM' 'KI' 'LU' 'OV' 'U']\n",
      "['BC' 'GBM' 'KI' 'LU' 'OV' 'U']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elinh\\OneDrive\\Skrivbord\\Big Data\\venvBigData\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Converts it to np.arrays and seperates labels form data\n",
    "TCGA = np.array(TCGAData['TCGA'])\n",
    "labels = np.array(TCGAData['TCGAclassstr'])\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "print(unique_labels)\n",
    "\n",
    "values = unice_labels\n",
    "print(values)\n",
    "# integer encode\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f32fb1",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc6f04",
   "metadata": {},
   "source": [
    "#### Method 1 (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ff29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pca(data, n_components):\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    # Fit PCA to the scaled data\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(scaled_data)\n",
    "    \n",
    "    # Transform the data using the fitted PCA model\n",
    "    principal_components = pca.transform(scaled_data)\n",
    "    \n",
    "    # Return the principal components and explained variance ratio\n",
    "    return principal_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e6a7e",
   "metadata": {},
   "source": [
    "#### Method 2 (Random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c0d05b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom_projection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianRandomProjection\n\u001b[1;32m----> 5\u001b[0m rp \u001b[38;5;241m=\u001b[39m GaussianRandomProjection(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m82\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(\u001b[43mdata\u001b[49m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Compute the explained variance ratio for each component\u001b[39;00m\n\u001b[0;32m      8\u001b[0m explained_variances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(rp\u001b[38;5;241m.\u001b[39mcomponents_)\u001b[38;5;241m.\u001b[39mvar(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(rp\u001b[38;5;241m.\u001b[39mcomponents_)\u001b[38;5;241m.\u001b[39mvar()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "rp = GaussianRandomProjection(n_components=82).fit(data)\n",
    "\n",
    "# Compute the explained variance ratio for each component\n",
    "explained_variances = np.abs(rp.components_).var(axis=1) / np.abs(rp.components_).var().sum()\n",
    "\n",
    "# Sort the components based on their explained variance\n",
    "sorted_indices = np.argsort(explained_variances)[::-1]\n",
    "explained_variances_sorted = explained_variances[sorted_indices]# Compute the explained variance ratio for each component\n",
    "explained_variances = np.abs(rp.components_).var(axis=1) / np.abs(rp.components_).var().sum()\n",
    "\n",
    "# Sort the components based on their explained variance\n",
    "sorted_indices = np.argsort(explained_variances)[::-1]\n",
    "explained_variances_sorted = explained_variances[sorted_indices]\n",
    "\n",
    "# Plot the scree plot\n",
    "plt.plot(np.arange(1, len(explained_variances) + 1), explained_variances_sorted, 'ok',markersize=2)\n",
    "plt.axhline(1, linestyle=\"dashed\", color=\"red\", linewidth=1)\n",
    "plt.xlabel('Component number')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.show()\n",
    "\n",
    "n_components = np.sum(explained_variances_sorted > 1)\n",
    "print(f'Number of components = {n_components}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvBigData",
   "language": "python",
   "name": "venvbigdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
