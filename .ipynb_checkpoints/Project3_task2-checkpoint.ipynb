{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa39830",
   "metadata": {},
   "source": [
    "## Task 2a\n",
    "Create cluster-irrelevant features in the data by e.g. perturbing features or replace with simulated noise features. Note, it is probably easier if you first filter out some cluster relevant features so you have an informative set to start from and keep these intact, but you are free to investigate the question of noisy features in other ways if you prefer.\n",
    "\n",
    "Investigate how an increasing proportion of cluster-irrelevant features impact clustering and the selection of the number of clusters. Is this sensitive to how you choose to reduce dimensions or select features?\n",
    "\n",
    "To make this interesting, explore at least 2-3 dimension reduction techniques. You can consider feature selection methods, projections, factorization methods or auto-encoders for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf463ea3",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f65fb21a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyreadr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyreadr\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m argmax\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyreadr'"
     ]
    }
   ],
   "source": [
    "import pyreadr\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Loads the data\n",
    "TCGAData = pyreadr.read_r('TCGAdata.RData') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae708fbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Converts it to np.arrays and seperates labels form data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(TCGAData[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTCGA\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(TCGAData[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTCGAclassstr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m unique_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#Converts it to np.arrays and seperates labels form data\n",
    "data = np.array(TCGAData['TCGA'])\n",
    "labels = np.array(TCGAData['TCGAclassstr'])\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "print(unique_labels)\n",
    "\n",
    "values = unice_labels\n",
    "print(values)\n",
    "# integer encode\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faac2d",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc42df5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m data_pp \u001b[38;5;241m=\u001b[39m StandardScaler()\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdata\u001b[49m)\n\u001b[0;32m      6\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(svd_solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(data_pp)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# The PCA object offers convenient ways of accessing the\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# principal directions and explained variance\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_pp = StandardScaler().fit_transform(data)\n",
    "pca = PCA(svd_solver=\"auto\").fit(data_pp)\n",
    "# The PCA object offers convenient ways of accessing the\n",
    "# principal directions and explained variance\n",
    "pca.components_;\n",
    "pca.explained_variance_;\n",
    "\n",
    "# To get the principal components, the `transform` method of the\n",
    "# PCA object can be used\n",
    "principal_components_pca = pca.transform(data_pp)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.plot(np.arange(1, len(pca.components_) + 1), pca.explained_variance_, 'ok', markersize=2)\n",
    "ax.axvline(6, linestyle=\"dashed\", color=\"red\", linewidth=1)\n",
    "ax.axhline(1, linestyle=\"dashed\", color=\"green\", linewidth=1)\n",
    "\n",
    "ax.set_xlabel(\"Principal Component\")\n",
    "ax.set_ylabel(\"Explained Variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pca(data, n_components):\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    # Fit PCA to the scaled data\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(scaled_data)\n",
    "    \n",
    "    # Transform the data using the fitted PCA model\n",
    "    principal_components = pca.transform(scaled_data)\n",
    "    \n",
    "    # Return the principal components and explained variance ratio\n",
    "    return principal_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1db927",
   "metadata": {},
   "source": [
    "### Random projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c757a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "rp = GaussianRandomProjection(n_components=82).fit(data)\n",
    "\n",
    "# Compute the explained variance ratio for each component\n",
    "explained_variances = np.abs(rp.components_).var(axis=1) / np.abs(rp.components_).var().sum()\n",
    "\n",
    "# Sort the components based on their explained variance\n",
    "sorted_indices = np.argsort(explained_variances)[::-1]\n",
    "explained_variances_sorted = explained_variances[sorted_indices]# Compute the explained variance ratio for each component\n",
    "explained_variances = np.abs(rp.components_).var(axis=1) / np.abs(rp.components_).var().sum()\n",
    "\n",
    "# Sort the components based on their explained variance\n",
    "sorted_indices = np.argsort(explained_variances)[::-1]\n",
    "explained_variances_sorted = explained_variances[sorted_indices]\n",
    "\n",
    "# Plot the scree plot\n",
    "plt.plot(np.arange(1, len(explained_variances) + 1), explained_variances_sorted, 'ok',markersize=2)\n",
    "plt.axhline(1, linestyle=\"dashed\", color=\"red\", linewidth=1)\n",
    "plt.xlabel('Component number')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.show()\n",
    "\n",
    "n_components = np.sum(explained_variances_sorted > 1)\n",
    "print(f'Number of components = {n_components}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_venv",
   "language": "python",
   "name": "bigdata_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
