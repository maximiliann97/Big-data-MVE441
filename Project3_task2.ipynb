{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa39830",
   "metadata": {},
   "source": [
    "## Task 2a\n",
    "Create cluster-irrelevant features in the data by e.g. perturbing features or replace with simulated noise features. Note, it is probably easier if you first filter out some cluster relevant features so you have an informative set to start from and keep these intact, but you are free to investigate the question of noisy features in other ways if you prefer.\n",
    "\n",
    "Investigate how an increasing proportion of cluster-irrelevant features impact clustering and the selection of the number of clusters. Is this sensitive to how you choose to reduce dimensions or select features?\n",
    "\n",
    "To make this interesting, explore at least 2-3 dimension reduction techniques. You can consider feature selection methods, projections, factorization methods or auto-encoders for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf463ea3",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65fb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Loads the data\n",
    "TCGAData = pyreadr.read_r('TCGAdata.RData') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae708fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts it to np.arrays and seperates labels form data\n",
    "data = np.array(TCGAData['TCGA'])\n",
    "labels = np.array(TCGAData['TCGAclassstr'])\n",
    "\n",
    "unique_labels = np.unique(labels)\n",
    "print(unique_labels)\n",
    "\n",
    "values = unique_labels\n",
    "print(values)\n",
    "# integer encode\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "print(np.unique(labels))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61faac2d",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23515e05",
   "metadata": {},
   "source": [
    "***PCA***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42df5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_pp = StandardScaler().fit_transform(data)\n",
    "pca = PCA(svd_solver=\"auto\").fit(data_pp)\n",
    "# The PCA object offers convenient ways of accessing the\n",
    "# principal directions and explained variance\n",
    "pca.components_;\n",
    "pca.explained_variance_;\n",
    "\n",
    "# To get the principal components, the `transform` method of the\n",
    "# PCA object can be used\n",
    "principal_components_pca = pca.transform(data_pp)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.plot(np.arange(1, len(pca.components_) + 1), pca.explained_variance_, 'ok', markersize=2)\n",
    "ax.axvline(40, linestyle=\"dashed\", color=\"red\", linewidth=1)\n",
    "ax.axhline(1, linestyle=\"dashed\", color=\"green\", linewidth=1)\n",
    "\n",
    "ax.set_xlabel(\"Principal Component\")\n",
    "ax.set_ylabel(\"Explained Variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pca(data, n_components):\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    # Fit PCA to the scaled data\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(scaled_data)\n",
    "    \n",
    "    # Transform the data using the fitted PCA model\n",
    "    principal_components = pca.transform(scaled_data)\n",
    "    \n",
    "    # Return the principal components and explained variance ratio\n",
    "    return principal_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1db927",
   "metadata": {},
   "source": [
    "***Random projection***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c757a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "rp = GaussianRandomProjection(n_components=82).fit(data)\n",
    "\n",
    "# Compute the explained variance ratio for each component\n",
    "explained_variances = np.abs(rp.components_).var(axis=1) / np.abs(rp.components_).var().sum()\n",
    "\n",
    "# Sort the components based on their explained variance\n",
    "sorted_indices = np.argsort(explained_variances)[::-1]\n",
    "explained_variances_sorted = explained_variances[sorted_indices]# Compute the explained variance ratio for each component\n",
    "explained_variances = np.abs(rp.components_).var(axis=1) / np.abs(rp.components_).var().sum()\n",
    "\n",
    "# Sort the components based on their explained variance\n",
    "sorted_indices = np.argsort(explained_variances)[::-1]\n",
    "explained_variances_sorted = explained_variances[sorted_indices]\n",
    "\n",
    "# Plot the scree plot\n",
    "plt.plot(np.arange(1, len(explained_variances) + 1), explained_variances_sorted, 'ok',markersize=2)\n",
    "plt.axhline(1, linestyle=\"dashed\", color=\"red\", linewidth=1)\n",
    "plt.xlabel('Component number')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.show()\n",
    "\n",
    "n_components = np.sum(explained_variances_sorted > 1)\n",
    "print(f'Number of components = {n_components}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38477abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_projection(data, nr_components):\n",
    "    scaled_data = StandardScaler().fit_transform(data)\n",
    "    return GaussianRandomProjection(n_components=nr_components).fit_transform(scaled_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d15cf",
   "metadata": {},
   "source": [
    "## Taking out the most relevant features to create a good dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eb08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume X is your numpy array\n",
    "correlation_matrix = np.corrcoef(data, rowvar=False)\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "# Where correlation is greater than threshold\n",
    "above_threshold_vars = {}\n",
    "\n",
    "for col in correlation_matrix.columns:\n",
    "    above_threshold_vars[col] = list(correlation_matrix.index[correlation_matrix[col] > threshold]) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdatavenv1",
   "language": "python",
   "name": "bigdatavenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
